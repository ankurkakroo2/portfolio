Built an audio-reactive 3D particle visualization from scratch. Like, properly from scratch. Microphone input, Web Audio API, physics simulation, the whole thing. Collaborative session with engineers on my team, just vibing and iterating. Started with a simple waveform line, ended with 20,000 particles forming an elastic mesh that ripples like fabric when you speak into it.

The journey was the fun part. Started with [Google's Anti-Gravity](https://antigravity.google/). Gemini's surprisingly good at UI stuff. Then bounced to Opus for the heavy optimization work. Morning session I tried [Open Code](https://opencode.ai/), another agentic terminal tool, just to feel out what different models bring to the table. Each tool had its strengths. Gemini for the initial visual intuition, Opus for the physics math and performance tuning.

Here's what made it click: I actually got to use physics. Real physics. Particle influence with inverse-distance falloff. Acceleration and deceleration curves. Wave propagation across a 2D grid. The whole "space-time continuum viewed from 45 degrees" thing started as a joke but became the actual architecture. Particles don't just move up and down. They influence neighbors, decay over time, respond to elasticity parameters. It's a connected system.

The iteration was messy in the best way. Started 1D (line), went 2D (plane), added cross-directional force propagation, doubled particle density multiple times, built a control panel with sliders for every parameter. At some point we were debugging why the particles looked "pointy" during sharp audio spikes. Turned out we needed broader activation patterns for smoother visual flow. The [final thing](https://github.com/ankurkakroo2/tremor) runs at 60fps with a 200x100 grid. Synthwave aesthetic, horizon perspective, depth fog. Actually beautiful.

What stuck: Different models see problems differently. And physics knowledge from years ago? Still useful. Still satisfying to apply. Good problems to have.
